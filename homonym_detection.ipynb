{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown, movie_reviews, treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training your own word2vec model using specific dataset \n",
    "def generateData():\n",
    "    sample_sentences = ['Because a truck flipped over, we have been stuck in stationary traffic for over three hours now.',\n",
    "                        'There was a stationery store at street level, with a doorway on one side with a sign above it that said `Elmar Dance Company, 2nd Floor.',\n",
    "                        'When the train engineer realized there was a stationary car on the tracks, he tried to stop the locomotives engine.',\n",
    "                        'A stationary train was standing in the platform','I went to a stationery shop to buy a bundle of pencils'\n",
    "                       ]\n",
    "    tokenized_sentence = [word_tokenize(x) for x in sample_sentences]\n",
    "    final_dataset = list(treebank.sents())\n",
    "    final_dataset.extend(list(brown.sents()))\n",
    "    final_dataset.extend(list(movie_reviews.sents()))\n",
    "    final_dataset.extend(tokenized_sentence)\n",
    "    return final_dataset\n",
    "\n",
    "def word2vec_model(final_dataset=list(), custom=False):\n",
    "    print('Inside the model loading module')\n",
    "    if custom and final_dataset:\n",
    "        model = Word2Vec(final_dataset,window=2,workers=4, min_count=2)\n",
    "    else:\n",
    "        # mention the path of downloaded google pretrained word2vec model or you can train your own model with your own data\n",
    "        # link\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format('./detecting_wsd/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "    print('Model loading complete')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the list of all possible homonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the list of all possible homonyms\n",
    "def homonyms_file_list():\n",
    "    \"\"\"\n",
    "    Return two dicts\n",
    "    homonyms_dict - a dict consisting of all the clubbed homonyms\n",
    "    reverse_homonym_dict - a dict consisting of every homonym as a index and value as a group id of homonym\n",
    "    \"\"\"\n",
    "    # specify the path of homonym list present\n",
    "    homonyms_data = pd.read_csv('./homonyms_list.csv', delimiter=',')\n",
    "    group_homonyms = homonyms_data.groupby(['relation_id', 'spelling'])\n",
    "    homonyms_dict = defaultdict(list)\n",
    "    reverse_homonym_dict = dict()\n",
    "    for x in list(group_homonyms['spelling']):\n",
    "        homonyms_dict[x[0][0]].append(x[0][1])\n",
    "        reverse_homonym_dict[x[0][1]] = x[0][0]\n",
    "    return homonyms_dict, reverse_homonym_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# input_sentence = 'I am standing near a stationary shop which is near to highway'\n",
    "# input_sentence = 'The effect of Persistent Sleepiness'\n",
    "# input_sentence = 'Downed Power Line effect PNM Customer'\n",
    "# input_sentence = 'I went two a shop to by a mobile phone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.pos_tag(word_tokenize(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main logic to calculate the wrong homonyms used in the input sentence\n",
    "\n",
    "def check_correct_homonym(model,input_sentence):\n",
    "    # load model\n",
    "    homonyms_dict, reverse_homonym_dict = homonyms_file_list()\n",
    "    tokenized_input = word_tokenize(input_sentence)\n",
    "    tagged_input = nltk.pos_tag(tokenized_input)\n",
    "    # filter all the words with a specific pos tags\n",
    "    tokenized_filtered_input = [x[0] for x in tagged_input if re.match(r'(^C|^J|^N|^R|^V|^I|^T|^W)',x[1])]\n",
    "    # words on which similarity is to be found\n",
    "    filter_tagged_words = [x[0] for x in tagged_input if re.match(r'(^N|^J)',x[1])]\n",
    "    best_fit_word_list = list()\n",
    "    for word in tokenized_filtered_input:\n",
    "        word = word.lower()\n",
    "        if word in reverse_homonym_dict:\n",
    "            found_word = word\n",
    "            # get the list of all possible homonyms words\n",
    "            possible_homonyms = homonyms_dict[reverse_homonym_dict[word.lower()]]\n",
    "            score_list = list()\n",
    "            homonym_score_dict = dict()\n",
    "            # need optimization\n",
    "            for x in possible_homonyms:\n",
    "                for y in filter_tagged_words:\n",
    "                    if y not in possible_homonyms:\n",
    "                        pass\n",
    "                        # do the calculation of similarity part\n",
    "                homonym_score_dict[x] = np.array(score_list).mean()\n",
    "            best_homonym = max(homonym_score_dict.items(), key=operator.itemgetter(1))[0]\n",
    "            if best_homonym==found_word:\n",
    "                flag,replace_word = True,found_word   \n",
    "            else: \n",
    "                flag,replace_word = False,best_homonym\n",
    "            best_fit_word_list.append((found_word, flag, replace_word))\n",
    "    # returns a list [(found_word, flag_to_replace_word, replace_word)]\n",
    "    return best_fit_word_list    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_word_replace_input(model):\n",
    "    input_sentence = ' Because a truck flipped over, we have been stuck in stationery traffic for over three hours now.'\n",
    "    homonyms_word_list = check_correct_homonym(model, input_sentence)\n",
    "    print('Input sentence : {}\\n'.format(input_sentence))\n",
    "    for homonym in homonyms_word_list:\n",
    "        if homonym[1]:\n",
    "            print('{} is correct in input sentence \\n '.format(homonym[0]))\n",
    "        else:\n",
    "            print('{found} is incorrect in input sentence and should be replaced with {replace} \\n'.format(found=homonym[0],replace=homonym[2]))\n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside the model loading module\n",
      "Model loading complete\n"
     ]
    }
   ],
   "source": [
    "model = word2vec_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence :  Because a truck flipped over, we have been stuck in stationery traffic for over three hours now.\n",
      "\n",
      "have is correct in input sentence \n",
      " \n",
      "been is incorrect in input sentence and should be replaced with bin \n",
      "\n",
      "in is correct in input sentence \n",
      " \n",
      "stationery is incorrect in input sentence and should be replaced with stationary \n",
      "\n",
      "for is correct in input sentence \n",
      " \n",
      "hours is correct in input sentence \n",
      " \n"
     ]
    }
   ],
   "source": [
    "find_word_replace_input(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:venv_conda]",
   "language": "python",
   "name": "conda-env-venv_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
